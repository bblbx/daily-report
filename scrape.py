#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥çƒ­ç‚¹æŠ“å–è„šæœ¬
æŠ“å– AIã€è·¨å¢ƒç”µå•†ã€äº§å“åˆ›ä¸šä¸‰ä¸ªé¢†åŸŸçš„çƒ­ç‚¹
"""

import json
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import re

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def fetch_hacker_news():
    """æŠ“å– Hacker News AI ç›¸å…³çƒ­ç‚¹"""
    try:
        url = "https://news.ycombinator.com/"
        resp = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        items = []
        
        for story in soup.select('.storyline')[:10]:
            title_el = story.select_one('.titleline a')
            if not title_el:
                continue
            title = title_el.text.strip()
            url = title_el.get('href', '')
            
            # è¿‡æ»¤ AI ç›¸å…³
            ai_keywords = ['AI', 'LLM', 'GPT', 'model', 'neural', 'deep learning', 'machine learning']
            if any(kw.lower() in title.lower() for kw in ai_keywords):
                items.append({
                    'title': title,
                    'source': 'Hacker News',
                    'hot': 'ğŸ”¥',
                    'url': url if url.startswith('http') else f'https://news.ycombinator.com/{url}'
                })
        
        return items[:5]
    except Exception as e:
        print(f"Hacker News æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_product_hunt():
    """æŠ“å– Product Hunt çƒ­é—¨äº§å“"""
    try:
        # Product Hunt æ²¡æœ‰å…¬å¼€ APIï¼Œç”¨ç®€åŒ–ç‰ˆ
        items = [
            {'title': 'æŸ¥çœ‹ä»Šæ—¥ Product Hunt çƒ­é—¨äº§å“', 'source': 'Product Hunt', 'hot': '#1', 'url': 'https://www.producthunt.com/'},
        ]
        return items
    except Exception as e:
        print(f"Product Hunt æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_36kr():
    """æŠ“å– 36Kr AI/åˆ›ä¸šæ–°é—»"""
    try:
        url = "https://36kr.com/"
        resp = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        items = []
        
        for article in soup.select('a[itemprop="url"]')[:15]:
            title = article.text.strip()
            if len(title) < 10 or len(title) > 50:
                continue
            
            # AI/åˆ›ä¸šå…³é”®è¯
            keywords = ['AI', 'å¤§æ¨¡å‹', 'èèµ„', 'åˆ›ä¸š', 'ç§‘æŠ€', 'äº’è”ç½‘']
            if any(kw in title for kw in keywords):
                items.append({
                    'title': title,
                    'source': '36Kr',
                    'hot': 'çƒ­é—¨',
                    'url': article.get('href', '')
                })
        
        return items[:5]
    except Exception as e:
        print(f"36Kr æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_indie_hackers():
    """æŠ“å– Indie Hackers çƒ­é—¨å¸–å­"""
    try:
        url = "https://www.indiehackers.com/"
        resp = requests.get(url, headers=HEADERS, timeout=10)
        items = []
        
        # ç®€åŒ–å¤„ç†
        if resp.status_code == 200:
            items.append({
                'title': 'æŸ¥çœ‹ Indie Hackers çƒ­é—¨åˆ›ä¸šæ•…äº‹',
                'source': 'Indie Hackers',
                'hot': 'ğŸ’¡',
                'url': 'https://www.indiehackers.com/'
            })
        
        return items
    except Exception as e:
        print(f"Indie Hackers æŠ“å–å¤±è´¥ï¼š{e}")
        return []

def fetch_cross_border_ecommerce():
    """æŠ“å–è·¨å¢ƒç”µå•†çƒ­ç‚¹ï¼ˆé›¨æœç½‘/AMZ123ï¼‰"""
    # è¿™äº›ç½‘ç«™åçˆ¬è¾ƒä¸¥ï¼Œè¿”å›å›ºå®šæ¨è
    items = [
        {'title': 'äºšé©¬é€Šæœ€æ–°æ”¿ç­–æ›´æ–°æ±‡æ€»', 'source': 'é›¨æœç½‘', 'hot': 'ğŸ“¢', 'url': 'https://www.cifnews.com/'},
        {'title': 'è·¨å¢ƒç”µå•†é€‰å“ç­–ç•¥æŒ‡å—', 'source': 'AMZ123', 'hot': 'å¹²è´§', 'url': 'https://www.amz123.com/'},
        {'title': 'ç‹¬ç«‹ç«™è¿è¥å®æ“åˆ†äº«', 'source': 'è·¨å¢ƒçŸ¥é“', 'hot': 'ğŸ”¥', 'url': 'https://www.kjws.net/'},
        {'title': 'æµ·å¤–è¥é”€æ¸ é“å¯¹æ¯”åˆ†æ', 'source': 'äº¿é‚¦åŠ¨åŠ›', 'hot': 'æ¨è', 'url': 'https://www.ebrun.com/'},
        {'title': 'è·¨å¢ƒç‰©æµæˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ', 'source': 'è·¨å¢ƒçœ¼', 'hot': 'å®ç”¨', 'url': 'https://www.kuajingyan.com/'}
    ]
    return items

def generate_data():
    """ç”Ÿæˆå®Œæ•´æ•°æ®"""
    ai_items = fetch_hacker_news() + fetch_product_hunt()
    ai_items = ai_items[:5]
    
    # å¦‚æœ AI çƒ­ç‚¹ä¸è¶³ 5 æ¡ï¼Œè¡¥å……é»˜è®¤
    default_ai = [
        {'title': 'AI è¡Œä¸šæœ€æ–°åŠ¨æ€', 'source': 'AI æ—¥æŠ¥', 'hot': 'ğŸ“Š', 'url': 'https://huggingface.co/blog'},
        {'title': 'å¤§æ¨¡å‹åº”ç”¨æ–°æ¡ˆä¾‹', 'source': 'Twitter', 'hot': 'ğŸ”¥', 'url': 'https://twitter.com/'},
    ]
    while len(ai_items) < 5:
        ai_items.append(default_ai[len(ai_items) % len(default_ai)])
    
    ecommerce_items = fetch_cross_border_ecommerce()
    
    startup_items = fetch_indie_hackers() + fetch_36kr()
    startup_items = startup_items[:5]
    
    default_startup = [
        {'title': 'SaaS åˆ›ä¸šç»éªŒåˆ†äº«', 'source': 'Indie Hackers', 'hot': 'ğŸ’°', 'url': 'https://www.indiehackers.com/'},
        {'title': 'æ–°äº§å“ä¸Šçº¿æ¡ˆä¾‹', 'source': 'Product Hunt', 'hot': 'ğŸš€', 'url': 'https://www.producthunt.com/'},
    ]
    while len(startup_items) < 5:
        startup_items.append(default_startup[len(startup_items) % len(default_startup)])
    
    data = {
        'ai': ai_items,
        'ecommerce': ecommerce_items,
        'startup': startup_items
    }
    
    return data

def write_js_file(data):
    """å†™å…¥ data.js æ–‡ä»¶"""
    js_content = f'''// æ¯æ—¥çƒ­ç‚¹æ•°æ® - è‡ªåŠ¨æ›´æ–°äº {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
const reportData = {json.dumps(data, ensure_ascii=False, indent=4)};
'''
    
    with open('data.js', 'w', encoding='utf-8') as f:
        f.write(js_content)
    
    print(f"æ•°æ®å·²æ›´æ–°ï¼š{len(data['ai'])} æ¡ AI + {len(data['ecommerce'])} æ¡ç”µå•† + {len(data['startup'])} æ¡åˆ›ä¸š")

if __name__ == '__main__':
    data = generate_data()
    write_js_file(data)
    print("âœ… çƒ­ç‚¹æŠ“å–å®Œæˆ")
